"""
–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—â–∏–π —Å–µ—Ä–≤–∏—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ k
"""

from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from scipy.fft import fft, fftfreq
from scipy.stats import variation, moment
from scipy.stats.mstats import gmean
import requests
import time
import os
import json
import joblib
from typing import Dict, List, Optional, Tuple, Union
import random
from scipy.stats import kurtosis, skew, entropy
import nolds  # –¥–ª—è —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
import antropy as ent  # –¥–ª—è —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã—Ö –º–µ—Ä
from dotenv import load_dotenv

# –§–∏–∫—Å–∏—Ä—É–µ–º –≤—Å–µ —Å–∏–¥—ã –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)
load_dotenv()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name()}")
    print(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")


class EvaluationMetrics:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""

    def __init__(self, horizons: List[int] = [1, 20]):
        self.horizons = sorted(horizons)
        self.max_horizon = max(horizons)

    def calculate_targets(self, df: pd.DataFrame, price_col: str = 'close') -> Dict[int, pd.Series]:
        """–†–∞—Å—á–µ—Ç —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        targets = {}
        for k in self.horizons:
            targets[k] = (df[price_col].shift(-k) / df[price_col] - 1)
        return targets

    def calculate_mae(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"""
        return np.mean(np.abs(y_true - y_pred))

    def calculate_brier(self, y_true: np.ndarray, prob_up: np.ndarray) -> float:
        """Brier score –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞"""
        binary_targets = (y_true > 0).astype(float)
        return np.mean((binary_targets - prob_up) ** 2)

    def calculate_direction_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """–î–æ–ª—è –≤–µ—Ä–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ (Direction Accuracy)"""
        correct_signs = np.sign(y_true) == np.sign(y_pred)
        return correct_signs.mean()

    def normalize_metrics(self, mae: float, brier: float,
                          mae_base: float, brier_base: float) -> Tuple[float, float]:
        """–ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –º–µ—Ç—Ä–∏–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–µ–π–∑–ª–∞–π–Ω–∞"""
        mae_norm = max(0, 1 - (mae / mae_base)) if mae_base > 0 else 0
        brier_norm = max(0, 1 - (brier / brier_base)) if brier_base > 0 else 0
        return mae_norm, brier_norm

    def calculate_final_score(self, y_true: np.ndarray, y_pred: np.ndarray,
                              prob_up: np.ndarray, mae_base: float, brier_base: float) -> Dict:
        """–†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–∫–æ—Ä–∞ –ø–æ —Ñ–æ—Ä–º—É–ª–µ"""

        mae = self.calculate_mae(y_true, y_pred)
        brier = self.calculate_brier(y_true, prob_up)
        da = self.calculate_direction_accuracy(y_true, y_pred)

        mae_norm, brier_norm = self.normalize_metrics(mae, brier, mae_base, brier_base)

        # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        da_component = 0.1 * (1 / da) if da > 0 else 0
        final_score = 0.7 * mae_norm + 0.3 * brier_norm + da_component

        return {
            'mae': mae,
            'brier': brier,
            'direction_accuracy': da,
            'mae_norm': mae_norm,
            'brier_norm': brier_norm,
            'final_score': final_score
        }


class BaselineModel:
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è Baseline –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ k
    """

    def __init__(self, horizons: List[int] = [1, 20], window_size: int = 5):
        self.horizons = sorted(horizons)
        self.window_size = window_size

    def compute_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–º–æ–º–µ–Ω—Ç—É–º, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)"""
        df_processed = df.copy()

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä–∞–º
        for ticker in df_processed['ticker'].unique():
            mask = df_processed['ticker'] == ticker
            ticker_data = df_processed[mask].copy()

            # 1. –ú–æ–º–µ–Ω—Ç—É–º = –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['momentum'] = (
                ticker_data['close'].pct_change(self.window_size)
            )

            # 2. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å = std –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['volatility'] = (
                ticker_data['close'].pct_change().rolling(self.window_size).std()
            )

            # 3. –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['ma'] = ticker_data['close'].rolling(self.window_size).mean()

            # 4. –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç MA (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)
            ticker_data['distance_from_ma'] = (
                    (ticker_data['close'] - ticker_data['ma']) / ticker_data['ma']
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            df_processed.loc[mask, 'momentum'] = ticker_data['momentum'].values
            df_processed.loc[mask, 'volatility'] = ticker_data['volatility'].values
            df_processed.loc[mask, 'ma'] = ticker_data['ma'].values
            df_processed.loc[mask, 'distance_from_ma'] = ticker_data['distance_from_ma'].values

        return df_processed

    def predict(self, df: pd.DataFrame) -> Tuple[np.ndarray, Dict[int, np.ndarray]]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤

        Returns:
            predictions: –º–∞—Å—Å–∏–≤ [return_k1, return_k2, ..., logit_k1, logit_k2, ...]
            prob_up_dict: —Å–ª–æ–≤–∞—Ä—å {k: prob_up_k}
        """
        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_with_features = self.compute_features(df)

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª—è–º–∏ (–¥–ª—è –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≥–¥–µ –Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏)
        df_with_features['momentum'] = df_with_features['momentum'].fillna(0)
        df_with_features['volatility'] = df_with_features['volatility'].fillna(0.01)
        df_with_features['distance_from_ma'] = df_with_features['distance_from_ma'].fillna(0)

        predictions_list = []
        prob_up_dict = {}

        for k in self.horizons:
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –º–æ–º–µ–Ω—Ç—É–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            scaling_factor = min(1.0, k / 5.0)  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ —Å–∏–ª—å–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç

            returns_k = df_with_features['momentum'] * scaling_factor

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
            def sigmoid(x, sensitivity=10):
                return 1 / (1 + np.exp(-sensitivity * x))

            # –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            sensitivity = max(5, 15 - k)  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –º–µ–Ω—å—à–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            prob_up_k = sigmoid(df_with_features['momentum'], sensitivity=sensitivity)

            # Clipping: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0.1, 0.9] –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            prob_up_k = prob_up_k.clip(0.1, 0.9)

            # Clipping: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑—É–º–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            max_return = min(0.5, k * 0.05)  # –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            returns_k = returns_k.clip(-max_return, max_return)

            predictions_list.extend([returns_k])
            prob_up_dict[k] = prob_up_k

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        all_returns = np.column_stack(predictions_list)

        # –î–æ–±–∞–≤–ª—è–µ–º logits –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é
        all_logits = []
        for k in self.horizons:
            prob_up_k = prob_up_dict[k]
            logit_k = np.log(prob_up_k / (1 - prob_up_k))
            all_logits.append(logit_k)

        all_logits = np.column_stack(all_logits)
        predictions = np.column_stack([all_returns, all_logits])

        return predictions, prob_up_dict

    @staticmethod
    def naive_forecast(df: pd.DataFrame, horizon: int = 20) -> Tuple[np.ndarray, np.ndarray]:
        """–ù–∞–∏–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑: —Å—Ä–µ–¥–Ω—è—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å"""
        returns = df['close'].pct_change().dropna()
        avg_return = returns.mean() if len(returns) > 0 else 0

        y_pred = np.full(len(df), avg_return)
        prob_up = np.full(len(df), 0.5)

        return y_pred, prob_up


class OpenRouterNewsProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ OpenRouter API —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º temperature=0"""

    def __init__(self, api_key: str = None, model: str = "deepseek/deepseek-v3.2-exp"):
        self.api_key = api_key or os.getenv('API_KEY_OPENROUTER')
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.model = model
        self.embedding_dim = 1024
        self.delay_between_requests = 0.2
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è OpenRouter
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
        }

        print(f"üì∞ –ù–∞—Å—Ç—Ä–æ–µ–Ω OpenRouter –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –º–æ–¥–µ–ª—å—é: {model}")
        print(f"   ‚úÖ Temperature —è–≤–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ 0 –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏")

    def get_news_embeddings(self, texts: List[str], batch_size: int = 20) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ OpenRouter API —Å temperature=0"""
        if len(texts) == 0:
            return np.zeros((0, self.embedding_dim))

        embeddings = []
        failed_count = 0
        total_batches = (len(texts) + batch_size - 1) // batch_size

        print(f"üì° –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ OpenRouter...")
        print(f"   –ú–æ–¥–µ–ª—å: {self.model}, –¢–µ–∫—Å—Ç–æ–≤: {len(texts)}, –ë–∞—Ç—á–µ–π: {total_batches}")
        print(f"   ‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: temperature=0 (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏)")

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_num = i // batch_size + 1

            try:
                # –Ø–í–ù–û–ï –£–ö–ê–ó–ê–ù–ò–ï temperature=0
                payload = {
                    "model": self.model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "–í—ã–ø–æ–ª–Ω—è–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–∞—Å–∞—é—â–∏—Ö—Å—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. "
                                       "–ù–∏–∫–∞–∫–æ–π –ª–∏—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—É–∏–∏, —Ç–æ–ª—å–∫–æ —ç–º–±—Ä–µ–¥–∏–Ω–≥ —Å–ª–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, "
                                       "–∫–∞—Å–∞—é—â–∏—Ö—Å—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.  –û—Ç–≤–µ—Ç —Ç—ã —Å–æ–±–∏—Ä–∞–µ—à—å –≤ –≤–∞–ª–∏–¥–Ω—ã–π "
                                       "–º–∞—Å—Å–∏–≤. –¢–æ –µ—Å—Ç—å –≤—ã–≤–æ–¥–∏ –º–∞—Å—Å–∏–≤ —á–∏—Å–µ–ª, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å "
                                       "–≤ python –º–∞—Å—Å–∏–≤ –∏ –Ω–∏–∫–∞–∫–æ–≥–æ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ù—É–ª–µ–≤—É–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–ø–∏—Å—ã–≤–∞–π –≤ "
                                       "–≤–∏–¥–µ 0, –∞ –Ω–µ 000 –∏–ª–∏ –∏–Ω–∞—á–µ. –î—Ä–æ–±–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É"
                        },
                        {
                            "role": "user",
                            "content": batch_texts[0]
                        }
                    ],
                    "temperature": 0  # ‚úÖ –Ø–í–ù–û –§–ò–ö–°–ò–†–£–ï–ú –î–õ–Ø –í–û–°–ü–†–û–ò–ó–í–û–î–ò–ú–û–°–¢–ò
                }

                print("post –∑–∞–ø—Ä–æ—Å")
                response = requests.post(
                    url=self.api_url,
                    headers=self.headers,
                    json=payload,
                )

                if response.status_code == 200:
                    print("–ø–æ–ª—É—á–µ–Ω–∏–µ json")
                    batch_data = response.json()
                    print('OK')
                    if 'choices' in batch_data:
                        print(batch_data['choices'][0]['message'])
                        try:
                            batch_embeddings = [item for item in json.loads(batch_data['choices'][0]['message']['content'])]
                            embeddings.extend(batch_embeddings)
                            print(f"   ‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –æ–±—Ä–∞–±–æ—Ç–∞–Ω ({len(batch_texts)} —Ç–µ–∫—Å—Ç–æ–≤)")
                        except requests.exceptions.RequestException as e:
                            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞—Ç—á–µ {batch_num}: {e}")
                    else:
                        print(f"   ‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞—Ç—á–µ {batch_num}")
                        embeddings.extend([np.zeros(self.embedding_dim) for _ in batch_texts])
                        failed_count += len(batch_texts)
                else:
                    error_msg = response.text[:200]
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ API –≤ –±–∞—Ç—á–µ {batch_num} ({response.status_code}): {error_msg}")
                    embeddings.extend([np.zeros(self.embedding_dim) for _ in batch_texts])
                    failed_count += len(batch_texts)

                time.sleep(self.delay_between_requests)

            except requests.exceptions.RequestException as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –≤ –±–∞—Ç—á–µ {batch_num}: {e}")
                embeddings.extend([np.zeros(self.embedding_dim) for _ in batch_texts])
                failed_count += len(batch_texts)

        if failed_count > 0:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å {failed_count} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {len(texts)}")
        else:
            print(f"‚úÖ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã!")

        return np.array(embeddings)


class HybridTransformerModel(nn.Module):
    """–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å: Transformer + LLM + –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""

    def __init__(self, ts_input_size: int, llm_embedding_dim: int, horizons: List[int] = [1, 20],
                 hidden_size: int = 256):
        super().__init__()
        self.horizons = sorted(horizons)
        self.num_horizons = len(horizons)

        # –ö–∞–∂–¥—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç 2 –≤—ã—Ö–æ–¥–∞: return –∏ logit
        output_size = 2 * self.num_horizons

        self.ts_transformer = TimeSeriesTransformer(
            input_size=ts_input_size,
            d_model=128,
            nhead=8,
            num_layers=3
        )

        self.llm_projection = nn.Linear(llm_embedding_dim, 128)

        total_input_size = 128 + 128

        self.fusion_network = nn.Sequential(
            nn.Linear(total_input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, output_size)
        )

    def forward(self, timeseries, news_embeddings):
        ts_features = self.ts_transformer(timeseries)
        news_features = self.llm_projection(news_embeddings)
        combined = torch.cat([ts_features, news_features], dim=1)
        output = self.fusion_network(combined)
        return output


class TimeSeriesDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""

    def __init__(self, features, news_embeddings, targets_dict: Dict[int, np.ndarray], seq_length=30):
        self.features = features
        self.news_embeddings = news_embeddings
        self.targets_dict = targets_dict
        self.horizons = sorted(targets_dict.keys())
        self.seq_length = seq_length

    def __len__(self):
        return len(self.features) - self.seq_length

    def __getitem__(self, idx):
        features_seq = self.features[idx:idx + self.seq_length]
        news_embedding = self.news_embeddings[idx + self.seq_length]

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤
        targets = []
        for horizon in self.horizons:
            target = self.targets_dict[horizon][idx + self.seq_length]
            targets.append(target)

        return (
            torch.FloatTensor(features_seq),
            torch.FloatTensor(news_embedding),
            torch.FloatTensor(targets)
        )


class FinancialForecastService:
    """–°–µ—Ä–≤–∏—Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ k"""

    def __init__(self, model_dir: str = "model_artifacts", seq_length: int = 30,
                 k_days: List[int] = [1, 20]):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        self.seq_length = seq_length
        self.k_days = sorted(k_days)
        self.device = device

        self.scaler = StandardScaler()
        self.news_processor = OpenRouterNewsProcessor()
        self.feature_generator = FeatureGenerator()
        self.evaluator = EvaluationMetrics(horizons=self.k_days)
        self.baseline_model = BaselineModel(horizons=self.k_days, window_size=5)
        self.model = None

        self.feature_columns = None
        self.selected_features = None
        # –ë–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
        self.mae_base_dict = {k: 0.01 for k in self.k_days}
        self.brier_base_dict = {k: 0.25 for k in self.k_days}

        print(f"üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å–µ—Ä–≤–∏—Å —Å –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏: {self.k_days}")

    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞"""
        print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        all_features = []

        for ticker in df['ticker'].unique():
            ticker_data = df[df['ticker'] == ticker].copy().sort_values('begin')

            if len(ticker_data) < 30:
                continue

            prices = ticker_data['close'].values
            dates = pd.DatetimeIndex(ticker_data['begin'])

            ticker_features = self.feature_generator.create_features(prices, dates)

            ticker_features['ticker'] = ticker
            ticker_features['begin'] = dates

            all_features.append(ticker_features)

        if not all_features:
            return df

        features_df = pd.concat(all_features, ignore_index=True)

        result_df = pd.merge(df, features_df, on=['ticker', 'begin'], how='left')

        result_df = result_df.fillna(method='ffill').fillna(method='bfill')

        print(f"   ‚úì –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(features_df.columns) - 2} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return result_df

    def prepare_news_features(self, news_df: pd.DataFrame, candles_df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        print("üì∞ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...")

        if news_df is None or len(news_df) == 0:
            candles_df['news_embedding'] = [np.zeros(1024) for _ in range(len(candles_df))]
            return candles_df

        news_df['publish_date'] = pd.to_datetime(news_df['publish_date'])
        news_df['date'] = news_df['publish_date'].dt.date

        news_df['full_text'] = news_df['title'] + ". " + news_df['publication']

        grouped_news = news_df.groupby(['date'])['full_text'].apply(
            lambda x: ' '.join(x) if len(x) > 0 else ""
        ).reset_index()

        news_texts = grouped_news['full_text'].tolist()
        print(f"   üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(news_texts)} –≥—Ä—É–ø–ø –Ω–æ–≤–æ—Å—Ç–µ–π...")
        news_embeddings = self.news_processor.get_news_embeddings(news_texts)

        grouped_news['news_embedding'] = list(news_embeddings)

        candles_df['date'] = pd.to_datetime(candles_df['begin']).dt.date
        merged_df = pd.merge(
            candles_df,
            grouped_news,
            left_on=['ticker', 'date'],
            right_on=['date'],
            how='left'
        )

        mask = merged_df['news_embedding'].isna()
        merged_df.loc[mask, 'news_embedding'] = [np.zeros(1024) for _ in range(mask.sum())]

        return merged_df

    def prepare_targets(self, df: pd.DataFrame) -> Dict[int, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        targets_dict = {}
        for k in self.k_days:
            targets_dict[k] = (df['close'].shift(-k) / df['close'] - 1)
        return targets_dict

    def _calculate_baseline_metrics(self, df: pd.DataFrame) -> Tuple[Dict, Dict]:
        """–†–∞—Å—á–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        print("üìä –†–∞—Å—á–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤...")

        eval_df = df.tail(max(100, len(df) // 5)).copy()

        if len(eval_df) < 10:
            print("   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –±–µ–π–∑–ª–∞–π–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            return {k: 0.01 for k in self.k_days}, {k: 0.25 for k in self.k_days}

        mae_base_dict = {}
        brier_base_dict = {}

        for k in self.k_days:
            actual_return_col = f'actual_return_{k}d'
            eval_df[actual_return_col] = (eval_df['close'].shift(-k) / eval_df['close'] - 1)
            eval_df_clean = eval_df.dropna(subset=[actual_return_col])

            if len(eval_df_clean) == 0:
                print(f"   ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ {k}, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
                mae_base_dict[k] = 0.01
                brier_base_dict[k] = 0.25
                continue

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π baseline –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫
            baseline_predictions, baseline_probs_dict = self.baseline_model.predict(eval_df_clean)
            y_true = eval_df_clean[actual_return_col].values

            # –ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            horizon_idx = self.k_days.index(k)
            y_pred_baseline = baseline_predictions[:, horizon_idx]
            prob_up_baseline = baseline_probs_dict[k]

            mae_base = self.evaluator.calculate_mae(y_true, y_pred_baseline)
            brier_base = self.evaluator.calculate_brier(y_true, prob_up_baseline)

            mae_base_dict[k] = mae_base
            brier_base_dict[k] = brier_base

            print(f"   ‚úÖ –ì–æ—Ä–∏–∑–æ–Ω—Ç {k} –¥–Ω–µ–π: MAE={mae_base:.6f}, Brier={brier_base:.6f}")

        return mae_base_dict, brier_base_dict

    def select_features(self, X_train, y_train, X_val, y_val):
        """–û—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üéØ –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        self.selected_features, scores = FeatureSelector.forward_selection(
            X_train, y_train, X_val, y_val, max_features=50, min_improvement=0.001
        )

        return self.selected_features

    def train(self, candles_df: pd.DataFrame, news_df: pd.DataFrame, val_ratio: float = 0.2):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        print(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤: {self.k_days}...")

        candles_df = self.create_features(candles_df)
        targets_dict = self.prepare_targets(candles_df)

        full_df = self.prepare_news_features(news_df, candles_df)

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–∞—Ö
        target_columns = [f'target_{k}d' for k in self.k_days]
        for k in self.k_days:
            full_df[f'target_{k}d'] = targets_dict[k]
        full_df = full_df.dropna(subset=target_columns)

        # –†–∞—Å—á–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫
        self.mae_base_dict, self.brier_base_dict = self._calculate_baseline_metrics(full_df)

        numeric_columns = full_df.select_dtypes(include=[np.number]).columns.tolist()
        numeric_columns = [col for col in numeric_columns if col not in target_columns + ['news_embedding']]
        self.feature_columns = numeric_columns

        print(f"   –î–æ—Å—Ç—É–ø–Ω–æ {len(self.feature_columns)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        full_df = full_df.sort_values('begin')
        split_idx = int(len(full_df) * (1 - val_ratio))
        train_df = full_df.iloc[:split_idx]
        val_df = full_df.iloc[split_idx:]

        print(f"   Train samples: {len(train_df)}, Val samples: {len(val_df)}")

        X_train = train_df[self.feature_columns].values
        X_val = val_df[self.feature_columns].values

        self.scaler.fit(X_train)
        X_train_scaled = self.scaler.transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        y_train = train_df[f'target_{self.k_days[0]}d'].values
        y_val = val_df[f'target_{self.k_days[0]}d'].values

        self.selected_features = self.select_features(X_train_scaled, y_train, X_val_scaled, y_val)

        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(self.selected_features)} –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        news_embeddings = np.stack(full_df['news_embedding'].values)

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_targets_dict = {}
        val_targets_dict = {}
        for k in self.k_days:
            train_targets_dict[k] = train_df[f'target_{k}d'].values
            val_targets_dict[k] = val_df[f'target_{k}d'].values

        self.model = HybridTransformerModel(
            ts_input_size=len(self.selected_features),
            llm_embedding_dim=news_embeddings.shape[1],
            horizons=self.k_days,
            hidden_size=256
        ).to(self.device)

        self._train_model(train_df, val_df, train_targets_dict, val_targets_dict, news_embeddings)

        self._save_artifacts()

        print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    def _train_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,
                     train_targets_dict: Dict, val_targets_dict: Dict, news_embeddings: np.ndarray):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏"""
        print("üß† –û–±—É—á–µ–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏...")

        X_train = self.scaler.transform(train_df[self.feature_columns].values)
        X_train_selected = X_train[:, self.selected_features]

        train_news_embeddings = news_embeddings[:len(train_df)]

        train_dataset = TimeSeriesDataset(
            X_train_selected, train_news_embeddings,
            train_targets_dict, self.seq_length
        )
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.MSELoss()

        self.model.train()
        for epoch in range(20):
            total_loss = 0
            for batch_ts, batch_news, batch_targets in train_loader:
                batch_ts = batch_ts.to(self.device)
                batch_news = batch_news.to(self.device)
                batch_targets = batch_targets.to(self.device)

                optimizer.zero_grad()

                outputs = self.model(batch_ts, batch_news)

                # –†–∞–∑–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥—ã –Ω–∞ returns –∏ logits
                num_horizons = len(self.k_days)
                pred_returns = outputs[:, :num_horizons]
                # pred_logits = outputs[:, num_horizons:]  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∫–∞

                loss = criterion(pred_returns, batch_targets)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                total_loss += loss.item()

            if epoch % 5 == 0:
                val_loss = self._validate(val_df, val_targets_dict, news_embeddings[len(train_df):])
                print(f"   Epoch {epoch}, Train Loss: {total_loss / len(train_loader):.6f}, Val Loss: {val_loss:.6f}")

    def _validate(self, val_df: pd.DataFrame, val_targets_dict: Dict, val_news_embeddings: np.ndarray) -> float:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()

        X_val = self.scaler.transform(val_df[self.feature_columns].values)
        X_val_selected = X_val[:, self.selected_features]

        total_loss = 0
        criterion = nn.MSELoss()
        num_horizons = len(self.k_days)

        with torch.no_grad():
            for i in range(len(X_val_selected) - self.seq_length):
                ts_seq = torch.FloatTensor(X_val_selected[i:i + self.seq_length]).unsqueeze(0).to(self.device)
                news_embed = torch.FloatTensor(val_news_embeddings[i + self.seq_length]).unsqueeze(0).to(self.device)

                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã
                targets = []
                for k in self.k_days:
                    target_val = val_targets_dict[k][i + self.seq_length]
                    targets.append(target_val)
                batch_targets = torch.FloatTensor([targets]).to(self.device)

                output = self.model(ts_seq, news_embed)
                pred_returns = output[:, :num_horizons]

                loss = criterion(pred_returns, batch_targets)
                total_loss += loss.item()

        self.model.train()
        return total_loss / (len(X_val_selected) - self.seq_length)

    def predict(self, candles_df: pd.DataFrame, news_df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        if self.model is None:
            self._load_artifacts()

        print(f"üéØ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤: {self.k_days}...")

        candles_df = self.create_features(candles_df)
        full_df = self.prepare_news_features(news_df, candles_df)

        X = self.scaler.transform(full_df[self.feature_columns].values)
        X_selected = X[:, self.selected_features]

        news_embeddings = np.stack(full_df['news_embedding'].values)

        self.model.eval()
        predictions = []

        with torch.no_grad():
            for i in range(len(X_selected) - self.seq_length):
                ts_seq = torch.FloatTensor(X_selected[i:i + self.seq_length]).unsqueeze(0).to(self.device)
                news_embed = torch.FloatTensor(news_embeddings[i + self.seq_length]).unsqueeze(0).to(self.device)

                output = self.model(ts_seq, news_embed)
                predictions.append(output.cpu().squeeze().numpy())

        if not predictions:
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
            return pd.DataFrame()

        padding = [predictions[0] for _ in range(self.seq_length)]
        all_predictions = np.vstack(padding + predictions)

        result_df = full_df.iloc[:len(all_predictions)].copy()

        num_horizons = len(self.k_days)

        # –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ returns –∏ logits
        returns_predictions = all_predictions[:, :num_horizons]
        logits_predictions = all_predictions[:, num_horizons:]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
        for i, k in enumerate(self.k_days):
            result_df[f'pred_return_{k}d'] = returns_predictions[:, i]

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞ (–∏–∑ logits)
        for i, k in enumerate(self.k_days):
            prob_up = 1 / (1 + np.exp(-logits_predictions[:, i]))
            result_df[f'pred_prob_up_{k}d'] = prob_up

        # Clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        for k in self.k_days:
            max_return = min(0.5, k * 0.05)
            result_df[f'pred_return_{k}d'] = result_df[f'pred_return_{k}d'].clip(-max_return, max_return)
            result_df[f'pred_prob_up_{k}d'] = result_df[f'pred_prob_up_{k}d'].clip(0.1, 0.9)

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞
        latest_predictions = result_df.sort_values('begin').groupby('ticker').last().reset_index()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        output_columns = ['ticker', 'begin']
        for k in self.k_days:
            output_columns.extend([f'pred_return_{k}d', f'pred_prob_up_{k}d'])

        return latest_predictions[output_columns]

    def evaluate_predictions(self, candles_df: pd.DataFrame, predictions: pd.DataFrame) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        print("üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤...")

        evaluation_results = {}

        for k in self.k_days:
            return_col = f'pred_return_{k}d'
            prob_col = f'pred_prob_up_{k}d'

            if return_col not in predictions.columns or prob_col not in predictions.columns:
                print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ {k} –¥–Ω–µ–π")
                continue

            merged_df = candles_df.merge(
                predictions[['ticker', 'begin', return_col, prob_col]],
                on=['ticker', 'begin'],
                how='inner'
            )

            if len(merged_df) == 0:
                print(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ {k} –¥–Ω–µ–π")
                continue

            merged_df[f'actual_return_{k}d'] = (merged_df['close'].shift(-k) / merged_df['close'] - 1)
            valid_data = merged_df.dropna(subset=[f'actual_return_{k}d'])

            if len(valid_data) == 0:
                print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ {k} –¥–Ω–µ–π")
                continue

            y_true = valid_data[f'actual_return_{k}d'].values
            y_pred = valid_data[return_col].values
            prob_up = valid_data[prob_col].values

            mae_base = self.mae_base_dict.get(k, 0.01)
            brier_base = self.brier_base_dict.get(k, 0.25)

            scores = self.evaluator.calculate_final_score(y_true, y_pred, prob_up, mae_base, brier_base)

            evaluation_results[k] = {
                'horizon': k,
                **scores,
                'num_samples': len(valid_data)
            }

            print(f"   ‚úÖ –ì–æ—Ä–∏–∑–æ–Ω—Ç {k} –¥–Ω–µ–π:")
            print(f"      Final Score: {scores['final_score']:.4f}")
            print(f"      MAE: {scores['mae']:.6f} (norm: {scores['mae_norm']:.4f})")
            print(f"      Brier: {scores['brier']:.6f} (norm: {scores['brier_norm']:.4f})")
            print(f"      Direction Accuracy: {scores['direction_accuracy']:.4f}")
            print(f"      –û–±—Ä–∞–∑—Ü–æ–≤: {len(valid_data)}")

        return evaluation_results

    def _save_artifacts(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—è –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã"""
        artifacts = {
            'model_state_dict': self.model.state_dict(),
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'selected_features': self.selected_features,
            'seq_length': self.seq_length,
            'k_days': self.k_days,  # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã
            'mae_base_dict': self.mae_base_dict,
            'brier_base_dict': self.brier_base_dict
        }

        torch.save(artifacts, self.model_dir / 'model_artifacts.pth')
        print("üíæ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã (–≤–∫–ª—é—á–∞—è –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –∏ –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏)")

    def _load_artifacts(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—è –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã"""
        artifacts_path = self.model_dir / 'model_artifacts.pth'
        if not artifacts_path.exists():
            raise FileNotFoundError(f"–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {artifacts_path}")

        artifacts = torch.load(artifacts_path, map_location=self.device)

        self.scaler = artifacts['scaler']
        self.feature_columns = artifacts['feature_columns']
        self.selected_features = artifacts['selected_features']
        self.seq_length = artifacts['seq_length']
        self.k_days = artifacts.get('k_days', [1, 20])  # ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã
        self.mae_base_dict = artifacts.get('mae_base_dict', {k: 0.01 for k in self.k_days})
        self.brier_base_dict = artifacts.get('brier_base_dict', {k: 0.25 for k in self.k_days})

        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        self.evaluator = EvaluationMetrics(horizons=self.k_days)
        self.baseline_model = BaselineModel(horizons=self.k_days, window_size=5)

        news_embeddings_dummy = np.zeros((1, 1024))
        self.model = HybridTransformerModel(
            ts_input_size=len(self.selected_features),
            llm_embedding_dim=news_embeddings_dummy.shape[1],
            horizons=self.k_days,  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã
            hidden_size=256
        ).to(self.device)

        self.model.load_state_dict(artifacts['model_state_dict'])
        print(f"üíæ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (–≥–æ—Ä–∏–∑–æ–Ω—Ç—ã: {self.k_days})")


def load_data(candles_paths: List[str], news_paths: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤"""
    candles_dfs = []
    for path in candles_paths:
        if Path(path).exists():
            df = pd.read_csv(path)
            df['begin'] = pd.to_datetime(df['begin'])
            candles_dfs.append(df)
            print(f"   üìä –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ—Ç–∏—Ä–æ–≤–∫–∏: {path} ({len(df)} —Å—Ç—Ä–æ–∫)")
        else:
            print(f"   ‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

    candles_df = pd.concat(candles_dfs, ignore_index=True) if candles_dfs else pd.DataFrame()

    news_dfs = []
    for path in news_paths:
        if Path(path).exists():
            df = pd.read_csv(path)
            df['publish_date'] = pd.to_datetime(df['publish_date'])
            news_dfs.append(df)
            print(f"   üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–∏: {path} ({len(df)} —Å—Ç—Ä–æ–∫)")
        else:
            print(f"   ‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

    news_df = pd.concat(news_dfs, ignore_index=True) if news_dfs else pd.DataFrame()

    return candles_df, news_df


class FeatureGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""

    @staticmethod
    def create_features(prices: pd.Series, dates: pd.DatetimeIndex) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        prices_series = pd.Series(prices, index=dates)

        features = pd.DataFrame(index=dates)

        # –õ–∞–≥–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        for lag in range(1, 7):
            features[f'lag_{lag}'] = prices_series.shift(lag)
            features[f'returns_{lag}'] = prices_series.pct_change(lag)
            features[f'log_{lag}'] = np.log(prices_series.shift(lag))
            features[f'diff_{lag}'] = prices_series - prices_series.shift(lag)

        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        for window in [3, 5, 7, 14, 21, 30]:
            features[f'ma_{window}'] = prices_series.rolling(window).mean()
            features[f'std_{window}'] = prices_series.rolling(window).std()
            features[f'median_{window}'] = prices_series.rolling(window).median()
            features[f'min_{window}'] = prices_series.rolling(window).min()
            features[f'max_{window}'] = prices_series.rolling(window).max()
            features[f'range_{window}'] = prices_series.rolling(window).max() - prices_series.rolling(window).min()
            features[f'var_{window}'] = prices_series.rolling(window).var()

        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for alpha in [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]:
            features[f'ema_alpha_{alpha}'] = prices_series.ewm(alpha=alpha, adjust=False).mean()
            ema1 = prices_series.ewm(alpha=alpha, adjust=False).mean()
            ema2 = ema1.ewm(alpha=alpha, adjust=False).mean()
            features[f'dema_alpha_{alpha}'] = 2 * ema1 - ema2

        # RSI (Relative Strength Index)
        for period in [14, 21, 30]:
            delta = prices_series.diff()
            gain = delta.where(delta > 0, 0).rolling(period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
            rs = gain / loss
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä
        for k_period in [14, 21, 30]:
            low_min = prices_series.rolling(k_period).min()
            high_max = prices_series.rolling(k_period).max()
            features[f'stoch_k_{k_period}'] = 100 * ((prices_series - low_min) / (high_max - low_min))
            features[f'stoch_d_{k_period}'] = features[f'stoch_k_{k_period}'].rolling(3).mean()

        # Williams %R
        for period in [14, 21, 30]:
            lowest_low = prices_series.rolling(period).min()
            highest_high = prices_series.rolling(period).max()
            features[f'williams_r_{period}'] = -100 * ((highest_high - prices_series) / (highest_high - lowest_low))

        # Rate of Change (ROC)
        for period in [5, 10, 14, 21]:
            features[f'roc_{period}'] = ((prices_series - prices_series.shift(period)) / prices_series.shift(
                period)) * 100

        # Momentum
        for period in [5, 10, 14, 21]:
            features[f'momentum_{period}'] = prices_series - prices_series.shift(period)

        # MACD
        ema12 = prices_series.ewm(span=12, adjust=False).mean()
        ema26 = prices_series.ewm(span=26, adjust=False).mean()
        features['macd'] = ema12 - ema26
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # –ü–æ–ª–æ—Å—ã –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞
        for period in [20]:
            ma = prices_series.rolling(period).mean()
            std = prices_series.rolling(period).std()
            features[f'bb_upper_{period}'] = ma + (std * 2)
            features[f'bb_middle_{period}'] = ma
            features[f'bb_lower_{period}'] = ma - (std * 2)
            features[f'bb_width_{period}'] = (features[f'bb_upper_{period}'] - features[f'bb_lower_{period}']) / ma
            features[f'bb_position_{period}'] = (prices_series - features[f'bb_lower_{period}']) / (
                    features[f'bb_upper_{period}'] - features[f'bb_lower_{period}'])

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['hour'] = dates.hour
        features['dayofweek'] = dates.dayofweek
        features['dayofmonth'] = dates.day
        features['month'] = dates.month
        features['quarter'] = dates.quarter

        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
        features['dayofweek_sin'] = np.sin(2 * np.pi * features['dayofweek'] / 7)
        features['dayofweek_cos'] = np.cos(2 * np.pi * features['dayofweek'] / 7)
        features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)
        features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)

        # –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['is_weekend'] = features['dayofweek'].isin([5, 6]).astype(int)
        features['is_month_start'] = (features['dayofmonth'] == 1).astype(int)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–º–µ–Ω—Ç—ã
        for window in [10, 20, 50]:
            features[f'skew_{window}'] = prices_series.rolling(window).apply(
                lambda x: skew(x) if len(x) > 2 else np.nan)
            features[f'kurtosis_{window}'] = prices_series.rolling(window).apply(
                lambda x: kurtosis(x) if len(x) > 3 else np.nan)
            features[f'cv_{window}'] = prices_series.rolling(window).apply(
                lambda x: variation(x) if len(x) > 1 and np.std(x) > 0 else np.nan)

        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π)
        for window in [64]:
            features[f'spectral_mean_{window}'] = prices_series.rolling(window).apply(
                lambda x: np.mean(np.abs(fft(x))) if len(x) == window else np.nan
            )

        return features


class FeatureSelector:
    """–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é Forward Selection"""

    @staticmethod
    def forward_selection(X_train, y_train, X_val, y_val, max_features=50, min_improvement=0.001):
        """Forward Selection –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

        n_features = X_train.shape[1]
        selected_features = []
        remaining_features = list(range(n_features))
        scores = []
        prev_score = -np.inf

        print(f"üéØ –ù–∞—á–∞–ª–æ Forward Selection (–º–∞–∫—Å–∏–º—É–º {max_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")

        for step in range(min(max_features, n_features)):
            best_score = -np.inf
            best_feature = None

            # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏
            for feature in remaining_features:
                # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—É—â–∏–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                current_features = selected_features + [feature]

                # –û–±—É—á–∞–µ–º –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é
                model = LinearRegression()
                model.fit(X_train[:, current_features], y_train)

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                score = model.score(X_val[:, current_features], y_val)

                if score > best_score:
                    best_score = score
                    best_feature = feature

            if prev_score != -np.inf:
                improvement = best_score - prev_score
            else:
                improvement = best_score

            # –î–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏–π –ø—Ä–∏–∑–Ω–∞–∫
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
            scores.append(best_score)

            print(f"   –®–∞–≥ {step + 1}: –¥–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫ {best_feature}, R¬≤ = {best_score:.4f}")

            # –£—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if improvement < min_improvement and step > 10:  # –ú–∏–Ω–∏–º—É–º 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                print(f"   ‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞: —É–ª—É—á—à–µ–Ω–∏–µ –º–µ–Ω–µ–µ {min_improvement * 100:.1f}%")
                break

            prev_score = best_score

        print(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return selected_features, scores


class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Transformer"""

    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class TimeSeriesTransformer(nn.Module):
    """Transformer –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""

    def __init__(self, input_size, d_model=64, nhead=8, num_layers=3, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.input_projection(x) * np.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.transformer(x)
        x = self.dropout(x)
        return x[:, -1, :]


class HybridTransformerModel(nn.Module):
    """–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å: Transformer + LLM + –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏"""

    def __init__(self, ts_input_size, llm_embedding_dim, hidden_size=256, output_size=4):
        super().__init__()

        self.ts_transformer = TimeSeriesTransformer(
            input_size=ts_input_size,
            d_model=128,
            nhead=8,
            num_layers=3
        )

        self.llm_projection = nn.Linear(llm_embedding_dim, 128)

        total_input_size = 128 + 128

        self.fusion_network = nn.Sequential(
            nn.Linear(total_input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, output_size)
        )

    def forward(self, timeseries, news_embeddings):
        ts_features = self.ts_transformer(timeseries)
        news_features = self.llm_projection(news_embeddings)
        combined = torch.cat([ts_features, news_features], dim=1)
        output = self.fusion_network(combined)
        return output


class TimeSeriesDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""

    def __init__(self, features, news_embeddings, targets_1d, targets_20d, seq_length=30):
        self.features = features
        self.news_embeddings = news_embeddings
        self.targets_1d = targets_1d
        self.targets_20d = targets_20d
        self.seq_length = seq_length

    def __len__(self):
        return len(self.features) - self.seq_length

    def __getitem__(self, idx):
        features_seq = self.features[idx:idx + self.seq_length]
        news_embedding = self.news_embeddings[idx + self.seq_length]
        target_1d = self.targets_1d[idx + self.seq_length]
        target_20d = self.targets_20d[idx + self.seq_length]

        return (
            torch.FloatTensor(features_seq),
            torch.FloatTensor(news_embedding),
            torch.FloatTensor([target_1d]),
            torch.FloatTensor([target_20d])
        )


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
    import argparse

    parser = argparse.ArgumentParser(description='–°–µ—Ä–≤–∏—Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--mode', type=str, required=True, choices=['train', 'predict', 'evaluate'],
                        help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: train, predict –∏–ª–∏ evaluate')
    parser.add_argument('--candles', type=str, nargs='+', required=True,
                        help='–ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏ (candles.csv, candles_2.csv, etc)')
    parser.add_argument('--news', type=str, nargs='+', required=True,
                        help='–ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ (news.csv, news_2.csv, etc)')
    parser.add_argument('--k_days', type=int, nargs='+', default=[1, 20],
                        help='–ì–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–Ω—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä: 1 5 20)')
    parser.add_argument('--output', type=str, default='predictions.csv',
                        help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏')
    parser.add_argument('--model_dir', type=str, default='model_artifacts',
                        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--evaluate', action='store_true',
                        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ—Ü–µ–Ω–∫—É –ø–æ—Å–ª–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')

    args = parser.parse_args()

    print("=" * 70)
    print("üöÄ –°–ï–†–í–ò–° –§–ò–ù–ê–ù–°–û–í–û–ì–û –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –° –ü–†–û–ò–ó–í–û–õ–¨–ù–´–ú–ò –ì–û–†–ò–ó–û–ù–¢–ê–ú–ò")
    print("=" * 70)
    print(f"üéØ –ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã: {args.k_days}")

    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    candles_df, news_df = load_data(args.candles, args.news)

    if candles_df.empty:
        print("‚ùå –ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫")
        return

    print(f"   ‚úì –í—Å–µ–≥–æ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫: {len(candles_df)}")
    print(f"   ‚úì –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_df) if not news_df.empty else 0}")
    print(f"   ‚úì –¢–∏–∫–µ—Ä—ã: {candles_df['ticker'].unique().tolist()}")

    # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏
    service = FinancialForecastService(model_dir=args.model_dir, k_days=args.k_days)

    if args.mode == 'train':
        print(f"\nüéØ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –î–õ–Ø –ì–û–†–ò–ó–û–ù–¢–û–í {args.k_days}...")
        service.train(candles_df, news_df)

    elif args.mode == 'predict':
        print(f"\nüéØ –ó–ê–ü–£–°–ö –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –î–õ–Ø –ì–û–†–ò–ó–û–ù–¢–û–í {args.k_days}...")
        predictions = service.predict(candles_df, news_df)

        if not predictions.empty:
            predictions.to_csv(args.output, index=False)
            print(f"üíæ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {args.output}")
            print(f"üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {list(predictions.columns)}")

            if args.evaluate:
                print("\nüìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò...")
                evaluation_results = service.evaluate_predictions(candles_df, predictions)

                if evaluation_results:
                    eval_df = pd.DataFrame([evaluation_results])
                    eval_df.to_csv('evaluation_results.csv', index=False)
                    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: evaluation_results.csv")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")

    elif args.mode == 'evaluate':
        print("\nüìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò...")
        if Path(args.output).exists():
            predictions = pd.read_csv(args.output)
            evaluation_results = service.evaluate_predictions(candles_df, predictions)

            if evaluation_results:
                eval_df = pd.DataFrame([evaluation_results])
                eval_df.to_csv('evaluation_results.csv', index=False)
                print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: evaluation_results.csv")
        else:
            print("‚ùå –§–∞–π–ª —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")

    print("\n" + "=" * 70)
    print("‚úÖ –í–´–ü–û–õ–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 70)


if __name__ == "__main__":
    main()
    # headers = {
    #     "Authorization": f"Bearer {os.getenv('API_KEY_OPENROUTER')}",
    # }
    # payload = {
    #     "model": "deepseek/deepseek-v3.2-exp",
    #     "messages": [
    #         {
    #             "role": "system",
    #             "content": "–í—ã–ø–æ–ª–Ω—è–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–∞—Å–∞—é—â–∏—Ö—Å—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. "
    #                        "–ù–∏–∫–∞–∫–æ–π –ª–∏—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—É–∏–∏, —Ç–æ–ª—å–∫–æ —ç–º–±—Ä–µ–¥–∏–Ω–≥ —Å–ª–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, "
    #                        "–∫–∞—Å–∞—é—â–∏—Ö—Å—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.  –û—Ç–≤–µ—Ç —Ç—ã —Å–æ–±–∏—Ä–∞–µ—à—å –≤ –≤–∞–ª–∏–¥–Ω—ã–π "
    #                        "–º–∞—Å—Å–∏–≤. –¢–æ –µ—Å—Ç—å –≤—ã–≤–æ–¥–∏ –º–∞—Å—Å–∏–≤ —á–∏—Å–µ–ª, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å "
    #                        "–≤ python –º–∞—Å—Å–∏–≤ –∏ –Ω–∏–∫–∞–∫–æ–≥–æ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞"
    #         },
    #         {
    #             "role": "user",
    #             "content": "–ö–ª—é—á–µ–≤—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–µ—Ñ—Ç–µ–≥–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–º–æ—Ç—Ä—è—Ç—Å—è –≤—ã–≥–æ–¥–Ω–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤. –¢–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤ –æ—Ç—Ä–∞—Å–ª–∏. –ö–ª—é—á–µ–≤—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–µ—Ñ—Ç–µ–≥–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–º–æ—Ç—Ä—è—Ç—Å—è –≤—ã–≥–æ–¥–Ω–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å —Ä–∞–∑–≤–∏—Ç—ã—Ö –∏ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è —Ä—ã–Ω–∫–æ–≤ –∫–∞–∫ –Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º, —Ç–∞–∫ –∏ –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º —É—Ä–æ–≤–Ω—è—Ö. –ú–æ–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å –∏ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—É—é –¥–æ–ª–≥–æ–≤—É—é –Ω–∞–≥—Ä—É–∑–∫—É, –≤—ã—Å–æ–∫—É—é —Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å, —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –¥–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫, –≤—ã—Å–æ–∫–∏–µ –¥–∏–≤–∏–¥–µ–Ω–¥—ã. –ö–∞–ø–∑–∞—Ç—Ä–∞—Ç—ã —Å–µ–∫—Ç–æ—Ä–∞ –≤—ã—Å–æ–∫–∏–µ, –æ–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—É—é –∏–Ω–≤–µ—Å—Ç–ø—Ä–æ–≥—Ä–∞–º–º—É \"–ì–∞–∑–ø—Ä–æ–º–∞\", –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤ —Ç.—á. –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –∫–ª—é—á–µ–≤–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ \"–°–∏–ª–∞ –°–∏–±–∏—Ä–∏\". 2019 –≥–æ–¥ —Å–ª–æ–∂–∏–ª—Å—è –Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –¥–æ–ª–ª–∞—Ä–æ–≤—ã—Ö —Ü–µ–Ω –Ω–∞ –Ω–µ—Ñ—Ç—å –∏ –≥–∞–∑, –æ–¥–Ω–∞–∫–æ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–º–æ–≥–ª–∏ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –¥–µ–≤–∞–ª—å–≤–∞—Ü–∏–∏ —Ä—É–±–ª—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –Ω–∞ –Ω–∞–ª–æ–≥–∞—Ö –∑–∞ —Å—á–µ—Ç –∏–∑ –º–µ–Ω–µ–Ω–Ω–æ–≥–æ –¥–µ–º–ø—Ñ–∏—Ä—É—é—â–µ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞. –û—Ç–¥–µ–ª—å–Ω–æ –æ—Ç–º–µ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π—Å—è –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –¥–∏–≤–∏–¥–µ–Ω–¥–Ω—ã—Ö –≤—ã–ø–ª–∞—Ç –≤ –æ—Ç—Ä–∞—Å–ª–∏.  –ü–æ–ª–æ–∂–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤  –ò—Å—Ç–æ—á–Ω–∏–∫: Bloomberg: –ü–°–ë –ê–Ω–∞–ª–∏—Ç –∏–∫–∞ & –°—Ç—Ä–∞—Ç–µ–≥–∏—è –†–∞–∑–º–µ—Ä —à–∞—Ä–∞ ‚Äì –í—ã—Ä—É—á–∫–∞, –º–ª—Ä–¥ –¥–æ–ª–ª. –ö–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è. –í–≤–µ–¥–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∞–∫—Ü–∏–∑–∞ –Ω–∞ –Ω–µ—Ñ—Ç—å (–¥–µ–º–ø—Ñ–∏—Ä—É—é—â–∏–π –º–µ—Ö–∞–Ω–∏–∑–º) –≤ –Ω–∞—á–∞–ª–µ –≥–æ–¥–∞ –±—ã–ª–æ —Å–æ–ø—Ä—è–∂–µ–Ω–æ —Å–æ —Å–ª–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —Ä—ã–Ω–æ—á–Ω—ã–º —Ä–µ–∞–ª–∏—è–º, –æ–¥–Ω–∞–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ–∑–≤–æ–ª–∏–ª–∞ –∫–æ–º–ø–∞–Ω–∏—è–º –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å. –ü–µ—Ä–µ—Ö–æ–¥ –∫ –Ω–æ–≤–æ–π –¥–∏–≤–∏–¥–µ–Ω–¥–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ ‚Äì \"–ì–∞–∑–ø—Ä–æ–º\" (50% –æ—Ç –ø—Ä–∏–±—ã–ª–∏ –ø–æ –ú–°–§–û –≤ 2021 –≥.) –∏ \"–õ–£–ö–û–ô–õ\" (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–∏–≤–∏–¥–µ–Ω–¥–∞–º, –∞ –Ω–µ –≤—ã–∫—É–ø—É —Å–≤–æ–∏—Ö –∞–∫—Ü–∏–π). –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–æ—Ä—ã  –ò—Å—Ç–æ—á–Ω–∏–∫: Bloomberg: –ü–°–ë –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ & –°—Ç—Ä–∞—Ç–µ–≥–∏—è –†–∏—Å–∫–∏. –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏ –æ—Ç—Ä–∞—Å–ª–∏ —Å–æ–ø—Ä—è–∂–µ–Ω—ã —Å –≤–æ–∑–º–æ–∂–Ω—ã–º –¥–∞–ª—å–Ω–µ–π—à–∏–º —É—Ö—É–¥—à–µ–Ω–∏–µ–º —Ü–µ–Ω–æ –≤–æ–π –∫–æ–Ω—ä—é–Ω–∫—Ç—É—Ä—ã –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –≤ —Å—Ñ–µ—Ä–µ –Ω–∞–ª–æ–≥–æ–æ–±–ª–æ–∂–µ–Ω–∏—è. –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–Ω–µ–∂–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤, –º–ª–Ω –¥–æ–ª–ª.  –ò—Å—Ç–æ—á–Ω–∏–∫: Bloomberg: –ü–°–ë –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ & –°—Ç—Ä–∞—Ç–µ–≥–∏—è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: \"–ì–∞–∑–ø—Ä–æ–º\". - –°–æ–≥–ª–∞—Å–Ω–æ –Ω–æ–≤–æ–π –¥–∏–≤–∏–¥–µ–Ω–¥–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ, \"–ì–∞–∑–ø—Ä–æ–º\" –ø–ª–∞–Ω–∏—Ä—É–µ—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 3 –ª–µ—Ç –¥–æ–≤–µ—Å—Ç–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏–≤–∏–¥–µ–Ω–¥–Ω—ã—Ö –≤—ã–ø–ª–∞—Ç –¥–æ 50%, –±–ª–∞–≥–æ–¥–∞—Ä—è —á–µ–º—É –¥–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—Å–∏—Ç—å 10%. - –ö–æ–º—Ñ–æ—Ä—Ç–Ω–∞—è –¥–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –±—É–¥–µ—Ç –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∏—é –¥–∏–≤–∏–¥–µ–Ω–¥–Ω—ã—Ö –≤—ã–ø–ª–∞—Ç. - –î–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –¥–ª—è –≤—ã–ø–ª–∞—Ç –¥–∏–≤–∏–¥–µ–Ω–¥–æ–≤ –∞–∫—Ü–∏–æ–Ω–µ—Ä–∞–º –æ–±—ä–µ–º FCF –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –Ω–∞—Ä–∞—â–∏–≤–∞–Ω–∏—è –∏–Ω–≤–µ—Å—Ç–ø—Ä–æ–≥—Ä–∞–º–º—ã. –î–∏–Ω–∞–º–∏–∫–∞ —Ü–µ–Ω –Ω–∞ –Ω–µ—Ñ—Ç—å –∏ –≥–∞–∑, –¥–æ–ª–ª./–º–ë–¢–ï  –ò—Å—Ç–æ—á–Ω–∏–∫: Bloomberg, –ü–°–ë –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ & –°—Ç—Ä–∞—Ç–µ–≥–∏—è"
    #         }
    #     ],
    #     "temperature": 0  # ‚úÖ –Ø–í–ù–û –§–ò–ö–°–ò–†–£–ï–ú –î–õ–Ø –í–û–°–ü–†–û–ò–ó–í–û–î–ò–ú–û–°–¢–ò
    # }
    # # print(batch_texts)
    # response = requests.post(
    #     url="https://openrouter.ai/api/v1/chat/completions",
    #     headers=headers,
    #     json=payload,
    # )
    #
    # if response.status_code == 200:
    #     print(response.status_code)
    #     print(response)
    #     batch_data = response.json()
    #     print('OK')
    #     if 'choices' in batch_data:
    #         batch_embeddings = batch_data['choices'][0]['message']['content']
    #         print(json.loads(batch_embeddings))
